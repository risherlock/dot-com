<!DOCTYPE html>

<html>
<head>
  <title>How I Spent My Winter Vacation</title>
  <link rel="stylesheet" href="../my_style.css">
</head>

<body>
<h1 class="title">
  Review: Cybernetics or control and communication in the animal and the machine
</h1>

<p class="date">
  John von Neumann<br>
  1949
</p>

<p>
  The first and most pregnant word of the title may not be illuminating, but the continuation gives the necessary lead at least to the technically prepared reader. The book deals with the automatic and quasi-automatic reactions of purposive mechanisms—artificial or living—to their surroundings, and more specifically, it deals with the regulatory mechanisms, the synthesis of which constitutes these automata. <q>Cybernetics</q> is a neologism, coined by the author for this occasion and—with a hope that many will share with him—for many future occasions. The author derived it from the Greek word nvflepvrirns (governor), that is, the <q>governor</q> of the steam engine, dealt with in an 1868 article of Clerk Maxwell: the first significant scientific paper on a relevant part of the theme of this book, guaranteeing, as it were, the intellectual pedigree of the subject.
</p>

<p>
  The author is one of the protagonists of the proposition that science, as well as technology, will in the near and in the farther future increasingly turn from problems of intensity, substance, and energy, to problems of structure, organization, information, and control. Any statement of this kind and generality is risky, inviting—and not quite innocently inviting—misinterpretation, and of dubious value outside of its technical context. It may, nevertheless, be important and valuable, if its true context exists and if it has a background of fruitful ideas. The reviewer believes that this context, this background of ideas, indeed exists in the case under consideration. Wiener's book constitutes an important systematic effort at exhibiting this context, and it may well be the first one where the subject and its potentialities receive the total emphasis that they deserve, where the magnitude of the program emerges in the proper perspective.
</p>

<p>
  The book's leading theme is the role of feedback mechanisms in purposive and control functions. <q>Feedback</q> is an engineering term indicating an arrangement which has the following properties: it observes the relationship of some mechanism to its surroundings, continuously senses the direction in which the controls of that mechanism have to be adjusted in order to bring it nearer to a certain desired relationship with those surroundings, makes the adjustment of these controls in the indicated direction, and thereby causes the mechanism gradually to find the desired position by this automatic procedure. Feedback mechanisms play a great role in numerous artificial automata and control instruments: in fact they are most characteristic of the newer phase of the development in these fields, and for many organs or groups of organs in living organisms it is very tempting to interpret their function in the same sense. Several students of the subject will feel that the importance of this particular phase of automat-organization has been overemphasized by the author. There is, however, a good deal to be said in favor of pressing home the <q>feedback</q> aspect at this time; it is a very characteristic one, and the freshness of the approach excuses some exaggeration in emphasis.
</p>

<p>
  The reviewer is inclined to take exception to the mathematical discussion of certain forms of randomness in the third chapter of the book, regarding some technical aspects of the presentation, and also regarding compatibility with the style and nature of the work as a whole. This excursus is probably too mathematical for many readers who are well qualified for the other parts of the book. On the other hand, the reader who disagrees or is lost in the technicalities can well concentrate his approval or his interest on the other parts of the book. They deserve both.
</p>

<p>
  The discussion of information and entropy touches upon a phase of a subject which has a fairly long earlier history. Entropy for the physicist is a concept belonging to the discipline of thermodynamics where the transformations among the various forms of energy are studied. It is well known that the total energy of a complete, <q>closed</q> system is always conserved: energy is neither created nor lost but only transformed. This constitutes the first fundamental theorem of thermodynamics or the energy theorem. There is, however, in addition, the second fundamental theorem of thermodynamics, or entropy theorem, which states that a hierarchy exists among the forms of energy: mechanical (kinetic or potential) energy, constituting the highest form, thermal energies constituting under it a decreasing hierarchical sequence in the order of decreasing temperature, and all other forms of energy permitting a complete classification relative to the gradations of this schema. It states, furthermore, that energy is always degraded, that is, that it always moves spontaneously from a higher form to a lower one, or if the opposite should happen in a part of the system, a compensatory degradation will have to take place in some other part. The bookkeeping that is required to account for this continuing overall degradation is effected by a certain well defined physical quantity, the entropy, which measures the hierachic position held or the degree of degradation suffered by any form of energy.
</p>

<p>
  The thermodynamical methods of measuring entropy were known in the mid-nineteenth century. Already in the early work on statistical physics (L. Boltzmann, 1896) it was observed that entropy was closely connected with information: Boltzmann found entropy to be proportional to the logarithm of the number of alternatives which are possible for a physical system after all the information that one possesses about that system macroscopically (that is, on the directly, humanly observable scale) has been recorded. In other words, it is proportional to the logarithm of the amount of missing information. This concept was elaborated further by various authors for various applications: H. Nyquist and R. V. L. Hartley, for transmission of information in the technical communication media (Bell System Technical Journal, Vol. 3, 1924, and Vol. 7, 1928) ; L. Szilard, for information in physics in general (Zschr. f. Phys., Vol. 53, 1929) ; and the reviewer, for quantum mechanics and elementary particle physics (Mathematical Foundations of Quantum Mechanics, Berlin, 1932, Chapter V).
</p>

<p>
  The technically well-equipped reader is advised to consult at this point some additional literature, primarily L. Szilard's work, referred to above, which also contains a particularly instructive analysis of the famous thermodynamical paradox of <q>Maxwell's Demon,</q> and C. R. Shannon's very important and interesting recent work on the <q>Theory of Information,</q> <q>Artificial Languages,</q> <q>Codes,</q> etc. (Bell Technical Journal, Vol. 27, 1948). There is reason to believe that the general degeneration laws, which hold when entropy is used as a measure of the hierarchic position of energy, have valid analogs when entropy is used as a measure of information. On this basis one may suspect the existence of connections between thermodynamics and new extensions of logics.
</p>

<p>
  The chapters on the connections between living organisms, the neural quasi-automaton, and artificial automata (e.g. computing machines) treat a dubious, difficult, and dangerously tempting subject with great virtuosity. It is also to be hoped that their references to the important work of W.S. McCulloch and W. Pitts (Bull. Math. Biophys., Vol. 5, 1943) will help to spread the ideas of these authors.
</p>

<p>
  The references of the book to certain important segments of the relevant contemporary literature are very complete, but it would have profited even more by further documentation of older literature on feedback processes, various physiological subjects, etc.
</p>

<p>
  It would transcend the reasonable limits of a review to attempt to account in any degree of detail for all the relevant aspects opened up in this book. A mention of some headings may suffice. Various major feedback mechanisms are discussed in varying degrees of detail. The relation of several concepts of randomness to each other and to our views concerning the physical world are analyzed. Interpretations of certain functions and malfunctions of living organisms are offered. The place at which modern computing automata fit into the larger picture is discussed. The nature of cooperative, as well as antagonistic, relationships in mechanisms, in society, and in artificial games is discussed. Various interesting references to the social implications of the ideas arising in the author's discussion are made.
</p>

<p>
  It is hoped that some feeling of the book's brilliancy, as well as of its bounds—-in summa, of its deeply original character—may have been conveyed in the above sketchy remarks. The book is a first, daring attack on what may well become a great and important subject, and the reader is not likely to read it and to part with it unmoved and unconvinced.
</p>

<hr class="footer">
<p class="footer">
  https://doi.org/10.1063/1.3066516<br>
  <a href="archives.html">archives</a>
</p>
</body>
</html>
